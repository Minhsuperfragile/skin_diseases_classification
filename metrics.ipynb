{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.load(r'model_eval/omg_pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.load(r'model_eval/omg_truth.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred = []\n",
    "for pred_ in pred:\n",
    "    for pred__ in pred_:\n",
    "        # because the output of a model is an array with predicted label having the highest value\n",
    "        _pred.append(np.argmax(pred__).item())\n",
    "len(_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_truth = truth.flatten()\n",
    "_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(_truth)\n",
    "\n",
    "# Initialize variables to store results\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Loop over each class and compute precision, recall, and F1-score\n",
    "for cls in classes:\n",
    "    # True Positives (TP): Both y_true and y_pred are cls\n",
    "    TP = np.sum((_truth == cls) & (_pred == cls))\n",
    "    \n",
    "    # False Positives (FP): _pred is cls, but _truth is not\n",
    "    FP = np.sum((_truth != cls) & (_pred == cls))\n",
    "    \n",
    "    # False Negatives (FN): _truth is cls, but _pred is not\n",
    "    FN = np.sum((_truth == cls) & (_pred != cls))\n",
    "    \n",
    "    # Precision: TP / (TP + FP)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    \n",
    "    # Recall: TP / (TP + FN)\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # F1-score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Store results\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Display the results for each class\n",
    "for i, cls in enumerate(classes):\n",
    "    print(f\"Class {cls}: Precision = {precision_scores[i]:.2f}, Recall = {recall_scores[i]:.2f}, F1-Score = {f1_scores[i]:.2f}\")\n",
    "\n",
    "# Average (Macro-Averaging)\n",
    "macro_precision = np.mean(precision_scores)\n",
    "macro_recall = np.mean(recall_scores)\n",
    "macro_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(f\"\\nMacro-Averaged Precision: {macro_precision:.2f}\")\n",
    "print(f\"Macro-Averaged Recall: {macro_recall:.2f}\")\n",
    "print(f\"Macro-Averaged F1-Score: {macro_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(_pred == _truth) / len(_truth)\n",
    "print(f\"Accuracy : {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(_truth, _pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Create the heatmap using seaborn\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(_truth), yticklabels=np.unique(_pred))\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
